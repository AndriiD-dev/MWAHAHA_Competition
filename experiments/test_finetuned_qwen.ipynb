{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-08T17:25:25.705209Z",
     "start_time": "2025-12-08T17:25:25.701862Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# If you are in Colab, adapt this, for local VS Code you can omit or set to repo root\n",
    "# os.chdir(\"/content/MWAHAHA_Competition\")\n",
    "\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "ADAPTER_PATH = \"../models/qwen_lora_jokes\" "
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T17:34:08.717299Z",
     "start_time": "2025-12-08T17:25:26.783917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a multilingual stand-up comedian. \"\n",
    "    \"You write short, original jokes in English\"\n",
    "    \"You ALWAYS obey the userâ€™s constraints exactly (word inclusion, topic, language). \"\n",
    "    \"You prefer concise setups and strong punchlines.\"\n",
    ")\n",
    "\n",
    "# Load tokenizer from adapter folder so it uses the same special tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model\n",
    "if torch.cuda.is_available():\n",
    "    dtype = torch.float16\n",
    "    device_map = \"auto\"\n",
    "    print(\"Using CUDA (float16)\")\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "    device_map = None\n",
    "    print(\"Using CPU (float32)\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Attach LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "model.eval()\n",
    "\n",
    "# (Optional) merge LoRA into base weights for slightly faster inference\n",
    "# model = model.merge_and_unload()\n",
    "# model.eval()\n"
   ],
   "id": "badb339a851d0189",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T17:49:34.409909Z",
     "start_time": "2025-12-08T17:49:34.408889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_chat_prompt(user_prompt: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    # add_generation_prompt=True adds the assistant role at the end for generation\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_joke(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 96,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.95,\n",
    "    do_sample: bool = True,\n",
    "):\n",
    "    text = build_chat_prompt(prompt)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Cut off the prompt part\n",
    "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return response.strip()\n"
   ],
   "id": "10100b3a2edc06cd",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T18:02:12.518979Z",
     "start_time": "2025-12-08T17:52:59.574116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_prompts = [\n",
    "    \"How do you collect lots of stars?\",\n",
    "    \"Tell me a short joke about programmers and coffee.\",\n",
    "    \"Make a pun about databases in one sentence.\",\n",
    "]\n",
    "\n",
    "for p in test_prompts:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"USER:\", p)\n",
    "    ans = generate_joke(p)\n",
    "    print(\"MODEL:\", ans)\n"
   ],
   "id": "728de2d385313618",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T18:08:14.656275Z",
     "start_time": "2025-12-08T18:02:19.024208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ans = generate_joke(\"How do you collect lots of stars?\",\n",
    "                    max_new_tokens=8,    # tiny\n",
    "                    do_sample=False)\n",
    "print(ans)\n"
   ],
   "id": "3da6469ff9abc1a",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T20:31:51.764289Z",
     "start_time": "2025-12-08T20:31:51.704378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "sft_result = pd.read_csv(\"../results/task-a-title_predictions.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "sft_result"
   ],
   "id": "bd11b87a5923cccc",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T20:44:32.453492Z",
     "start_time": "2025-12-08T20:44:32.436075Z"
    }
   },
   "cell_type": "code",
   "source": "print(sft_result.isna().sum())\n",
   "id": "4129f7690b5c2c62",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "25478e5448df2e98",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

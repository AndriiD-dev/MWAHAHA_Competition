{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-16T00:42:47.177797Z",
     "start_time": "2025-12-16T00:42:47.073399Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "WIKI_SUMMARY_URL = \"https://en.wikipedia.org/api/rest_v1/page/summary/{}\"\n",
    "WIKI_SEARCH_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# Put something descriptive here (Wikimedia expects this)\n",
    "WIKI_HEADERS = {\n",
    "    \"User-Agent\": \"MWAHAHA/1.0 (contact: dardemtum@gmail.com) humor-generation\"\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_one_line(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def safe_word(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Conservative sanitizer for Wikipedia titles:\n",
    "    - keeps letters, numbers, spaces, hyphens, apostrophes\n",
    "    - strips everything else\n",
    "    \"\"\"\n",
    "    s = normalize_one_line(s)\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"[^A-Za-z0-9 \\-']\", \"\", s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def looks_like_disambiguation(extract: str, page_type: str | None) -> bool:\n",
    "    t = (extract or \"\").strip().lower()\n",
    "    if page_type and page_type.strip().lower() == \"disambiguation\":\n",
    "        return True\n",
    "    if \"may refer to:\" in t:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def word_variants_for_wiki(word: str) -> List[str]:\n",
    "    w = safe_word(word)\n",
    "    if not w:\n",
    "        return []\n",
    "\n",
    "    variants = [w]\n",
    "    wl = w.lower()\n",
    "\n",
    "    # all caps noise\n",
    "    if w.isupper() and len(w) >= 4:\n",
    "        variants.append(w.title())\n",
    "        variants.append(w.lower())\n",
    "\n",
    "    # bullies -> bully\n",
    "    if wl.endswith(\"ies\") and len(wl) > 4:\n",
    "        variants.append(w[:-3] + \"y\")\n",
    "\n",
    "    # watches -> watch, boxes -> box\n",
    "    if wl.endswith(\"es\") and len(wl) > 4:\n",
    "        variants.append(w[:-2])\n",
    "\n",
    "    # astronauts -> astronaut\n",
    "    if wl.endswith(\"s\") and len(wl) > 3 and not wl.endswith(\"ss\"):\n",
    "        variants.append(w[:-1])\n",
    "\n",
    "    # deduplicate case-insensitive\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for v in variants:\n",
    "        key = v.lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "\n",
    "def fetch_summary(title: str) -> Tuple[str, str]:\n",
    "    t = safe_word(title)\n",
    "    if not t:\n",
    "        return (\"\", \"\")\n",
    "\n",
    "    url = WIKI_SUMMARY_URL.format(requests.utils.quote(t))\n",
    "    try:\n",
    "        r = requests.get(url, headers=WIKI_HEADERS, timeout=12)\n",
    "        if r.status_code != 200:\n",
    "            return (\"\", \"\")\n",
    "        data = r.json()\n",
    "        extract = normalize_one_line(data.get(\"extract\", \"\") or \"\")\n",
    "        page_type = normalize_one_line(data.get(\"type\", \"\") or \"\")\n",
    "        return (extract, page_type)\n",
    "    except Exception:\n",
    "        return (\"\", \"\")\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=4096)\n",
    "def search_titles(query: str) -> List[str]:\n",
    "    q = safe_word(query)\n",
    "    if not q:\n",
    "        return []\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": q,\n",
    "        \"format\": \"json\",\n",
    "        \"utf8\": 1,\n",
    "        \"srlimit\": 6,\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(WIKI_SEARCH_URL, params=params, headers=WIKI_HEADERS, timeout=12)\n",
    "        if r.status_code != 200:\n",
    "            return []\n",
    "        data = r.json()\n",
    "        items = (data.get(\"query\", {}) or {}).get(\"search\", []) or []\n",
    "        titles = []\n",
    "        for it in items:\n",
    "            title = normalize_one_line(it.get(\"title\", \"\") or \"\")\n",
    "            if title:\n",
    "                titles.append(title)\n",
    "        return titles\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=8192)\n",
    "def get_wikipedia_extract_cached(word: str) -> str:\n",
    "    w = safe_word(word)\n",
    "    if not w:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) direct try (with variants)\n",
    "    for cand in word_variants_for_wiki(w):\n",
    "        extract, page_type = fetch_summary(cand)\n",
    "        if not extract:\n",
    "            continue\n",
    "        if looks_like_disambiguation(extract, page_type):\n",
    "            continue\n",
    "        return extract\n",
    "\n",
    "    # 2) resolve via search (skip obvious disambiguation titles)\n",
    "    for q in word_variants_for_wiki(w):\n",
    "        for title in search_titles(q):\n",
    "            if title.lower().endswith(\"(disambiguation)\"):\n",
    "                continue\n",
    "            extract, page_type = fetch_summary(title)\n",
    "            if not extract:\n",
    "                continue\n",
    "            if looks_like_disambiguation(extract, page_type):\n",
    "                continue\n",
    "            return extract\n",
    "\n",
    "    return \"\"\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:42:47.977101Z",
     "start_time": "2025-12-16T00:42:47.927992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CACHE_IN = Path(\"wiki_extract_cache.json\")\n",
    "CACHE_OUT = Path(\"wiki_extract_cache_updated.json\")\n",
    "\n",
    "extracts = json.loads(CACHE_IN.read_text(encoding=\"utf-8\"))\n",
    "print(\"Loaded cache entries:\", len(extracts))\n",
    "\n",
    "def is_bad_extract(x: str) -> bool:\n",
    "    t = (x or \"\").strip().lower()\n",
    "    return (t == \"\") or (\"may refer to:\" in t)\n",
    "\n",
    "bad_words = [w for w, ex in extracts.items() if is_bad_extract(ex)]\n",
    "print(\"Bad entries to rerun:\", len(bad_words))\n",
    "print(\"Sample bad words:\", bad_words[:30])\n"
   ],
   "id": "5a394d2f721ce67f",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T02:11:42.092904Z",
     "start_time": "2025-12-16T00:45:31.142303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Inputs/outputs\n",
    "CACHE_IN = Path(\"wiki_extract_cache.json\")\n",
    "RERUN_RESULTS_JSON = Path(\"wiki_rerun_results.json\")\n",
    "RERUN_LOG_TSV = Path(\"wiki_rerun_log.tsv\")\n",
    "CACHE_OUT = Path(\"wiki_extract_cache_updated.json\")\n",
    "\n",
    "SLEEP_SECONDS = 0.20\n",
    "PRINT_EVERY = 50\n",
    "\n",
    "extracts = json.loads(CACHE_IN.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def is_bad_extract(x: str) -> bool:\n",
    "    t = (x or \"\").strip().lower()\n",
    "    return (t == \"\") or (\"may refer to:\" in t)\n",
    "\n",
    "bad_words = [w for w, ex in extracts.items() if is_bad_extract(ex)]\n",
    "print(\"Loaded cache entries:\", len(extracts))\n",
    "print(\"Bad entries to rerun:\", len(bad_words))\n",
    "\n",
    "\n",
    "def word_variants(word: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a small set of normalized variants to try.\n",
    "    We will use these variants as:\n",
    "    - direct summary title queries\n",
    "    - search queries (resolver)\n",
    "    \"\"\"\n",
    "    w = safe_word(word)\n",
    "    if not w:\n",
    "        return []\n",
    "\n",
    "    vars_ = [w]\n",
    "\n",
    "    # lower/title variants (helps for ALL CAPS or weird casing)\n",
    "    vars_.append(w.lower())\n",
    "    vars_.append(w.title())\n",
    "\n",
    "    wl = w.lower()\n",
    "\n",
    "    # plural / inflection heuristics\n",
    "    if wl.endswith(\"ies\") and len(wl) > 4:\n",
    "        vars_.append(w[:-3] + \"y\")\n",
    "\n",
    "    if wl.endswith(\"es\") and len(wl) > 4:\n",
    "        vars_.append(w[:-2])\n",
    "\n",
    "    if wl.endswith(\"s\") and len(wl) > 3 and not wl.endswith(\"ss\"):\n",
    "        vars_.append(w[:-1])\n",
    "\n",
    "    # de-duplicate case-insensitive\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for v in vars_:\n",
    "        v2 = safe_word(v)\n",
    "        if not v2:\n",
    "            continue\n",
    "        key = v2.lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(v2)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_extract_with_trace(word: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a trace dict:\n",
    "      {\n",
    "        \"input_word\": ...,\n",
    "        \"normalized_variants\": [...],\n",
    "        \"best_extract\": \"...\",\n",
    "        \"best_title\": \"...\",\n",
    "        \"method\": \"direct|search|none\",\n",
    "        \"status\": \"ok|empty|disambiguation\",\n",
    "      }\n",
    "    \"\"\"\n",
    "    variants = word_variants(word)\n",
    "    trace = {\n",
    "        \"input_word\": word,\n",
    "        \"normalized_variants\": variants,\n",
    "        \"best_extract\": \"\",\n",
    "        \"best_title\": \"\",\n",
    "        \"method\": \"none\",\n",
    "        \"status\": \"empty\",\n",
    "    }\n",
    "    if not variants:\n",
    "        return trace\n",
    "\n",
    "    # 1) Direct attempts with variants\n",
    "    for cand in variants:\n",
    "        ex, page_type = fetch_summary(cand)\n",
    "        ex = normalize_one_line(ex)\n",
    "        if not ex:\n",
    "            continue\n",
    "        if looks_like_disambiguation(ex, page_type):\n",
    "            trace[\"status\"] = \"disambiguation\"\n",
    "            continue\n",
    "\n",
    "        trace[\"best_extract\"] = ex\n",
    "        trace[\"best_title\"] = cand\n",
    "        trace[\"method\"] = \"direct\"\n",
    "        trace[\"status\"] = \"ok\"\n",
    "        return trace\n",
    "\n",
    "    # 2) Search resolver attempts\n",
    "    # Use each variant as a query; pick first non-disambiguation title with a real extract\n",
    "    for q in variants:\n",
    "        titles = search_titles(q)\n",
    "        for title in titles:\n",
    "            if title.lower().endswith(\"(disambiguation)\"):\n",
    "                continue\n",
    "            ex, page_type = fetch_summary(title)\n",
    "            ex = normalize_one_line(ex)\n",
    "            if not ex:\n",
    "                continue\n",
    "            if looks_like_disambiguation(ex, page_type):\n",
    "                trace[\"status\"] = \"disambiguation\"\n",
    "                continue\n",
    "\n",
    "            trace[\"best_extract\"] = ex\n",
    "            trace[\"best_title\"] = title\n",
    "            trace[\"method\"] = \"search\"\n",
    "            trace[\"status\"] = \"ok\"\n",
    "            return trace\n",
    "\n",
    "    # nothing found\n",
    "    return trace\n",
    "\n",
    "\n",
    "# Rerun and store separate results\n",
    "rerun_results = {}\n",
    "log_rows = []\n",
    "updated = 0\n",
    "still_bad = 0\n",
    "\n",
    "t0 = time.time()\n",
    "for i, w in enumerate(bad_words, start=1):\n",
    "    old_ex = normalize_one_line(extracts.get(w, \"\"))\n",
    "\n",
    "    trace = get_extract_with_trace(w)\n",
    "    new_ex = trace[\"best_extract\"]\n",
    "\n",
    "    # Save rerun result separately regardless of whether we update the main cache\n",
    "    rerun_results[w] = trace\n",
    "\n",
    "    # Decide if we should update main cache:\n",
    "    # update only if new extract is:\n",
    "    # - non-empty\n",
    "    # - not disambiguation\n",
    "    # - longer than old extract\n",
    "    should_update = bool(new_ex) and (\"may refer to:\" not in new_ex.lower()) and (len(new_ex) > len(old_ex))\n",
    "\n",
    "    if should_update:\n",
    "        extracts[w] = new_ex\n",
    "        updated += 1\n",
    "        action = \"UPDATED\"\n",
    "    else:\n",
    "        still_bad += 1\n",
    "        action = \"SKIPPED\"\n",
    "\n",
    "    log_rows.append({\n",
    "        \"word\": w,\n",
    "        \"action\": action,\n",
    "        \"old_len\": len(old_ex),\n",
    "        \"new_len\": len(new_ex),\n",
    "        \"method\": trace[\"method\"],\n",
    "        \"best_title\": trace[\"best_title\"],\n",
    "        \"status\": trace[\"status\"],\n",
    "        \"old_extract_preview\": old_ex[:120],\n",
    "        \"new_extract_preview\": new_ex[:120],\n",
    "    })\n",
    "\n",
    "    if i % PRINT_EVERY == 0:\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"{i}/{len(bad_words)} rerun (elapsed {elapsed:.1f}s). updated={updated}, skipped={still_bad}. example={w!r}\")\n",
    "        if trace[\"best_title\"]:\n",
    "            print(\"  best_title:\", trace[\"best_title\"])\n",
    "        if new_ex:\n",
    "            print(\"  new:\", new_ex[:200])\n",
    "\n",
    "    time.sleep(SLEEP_SECONDS)\n",
    "\n",
    "# Save rerun-only results (for later inspection)\n",
    "RERUN_RESULTS_JSON.write_text(json.dumps(rerun_results, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved rerun results:\", RERUN_RESULTS_JSON.resolve())\n",
    "\n",
    "# Save human-readable log\n",
    "import pandas as pd\n",
    "pd.DataFrame(log_rows).to_csv(RERUN_LOG_TSV, sep=\"\\t\", index=False)\n",
    "print(\"Saved rerun log:\", RERUN_LOG_TSV.resolve())\n",
    "\n",
    "# Save updated cache as a new file (does not overwrite original)\n",
    "CACHE_OUT.write_text(json.dumps(extracts, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"\\nDone.\")\n",
    "print(\"Updated entries:\", updated)\n",
    "print(\"Skipped (still bad or not improved):\", still_bad)\n",
    "print(\"Saved updated cache:\", CACHE_OUT.resolve())\n"
   ],
   "id": "756fe74ca1f89747",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T09:53:10.358908Z",
     "start_time": "2025-12-16T09:53:09.989386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Use your actual filenames\n",
    "ORIG_CACHE = Path(\"wiki_extract_cache.json\")\n",
    "UPDATED_CACHE = Path(\"wiki_extract_cache_updated.json\")\n",
    "\n",
    "CLEAN_UPDATED_OUT = Path(\"wiki_extract_cache_updated_cleaned.json\")\n",
    "MERGED_OUT = Path(\"wiki_extract_cache_merged.json\")\n",
    "\n",
    "def normalize_one_line(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def is_disambiguation(s: str) -> bool:\n",
    "    t = normalize_one_line(s).lower()\n",
    "    if not t:\n",
    "        return False\n",
    "    return (\"may refer to\" in t) or (\"commonly refers to\" in t)\n",
    "\n",
    "# Load\n",
    "orig = json.loads(ORIG_CACHE.read_text(encoding=\"utf-8\"))\n",
    "upd = json.loads(UPDATED_CACHE.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(\"Loaded original:\", len(orig), \"entries from\", ORIG_CACHE.resolve())\n",
    "print(\"Loaded updated :\", len(upd),  \"entries from\", UPDATED_CACHE.resolve())\n",
    "\n",
    "# 1) Clean updated: blank out disambiguation extracts\n",
    "cleaned_upd = {}\n",
    "blanked = 0\n",
    "for k, v in upd.items():\n",
    "    v_norm = normalize_one_line(v)\n",
    "    if is_disambiguation(v_norm):\n",
    "        cleaned_upd[k] = \"\"\n",
    "        blanked += 1\n",
    "    else:\n",
    "        cleaned_upd[k] = v_norm\n",
    "\n",
    "CLEAN_UPDATED_OUT.write_text(json.dumps(cleaned_upd, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"\\nSaved cleaned updated cache:\", CLEAN_UPDATED_OUT.resolve())\n",
    "print(\"Blanked disambiguation entries:\", blanked)\n",
    "\n",
    "# 2) Merge into original: replace only when updated value is non-empty\n",
    "merged = dict(orig)\n",
    "replaced = 0\n",
    "\n",
    "for k, v_new in cleaned_upd.items():\n",
    "    if not v_new:\n",
    "        continue\n",
    "    if k in merged and normalize_one_line(merged[k]) != v_new:\n",
    "        merged[k] = v_new\n",
    "        replaced += 1\n",
    "\n",
    "MERGED_OUT.write_text(json.dumps(merged, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"\\nSaved merged cache:\", MERGED_OUT.resolve())\n",
    "print(\"Replaced entries:\", replaced)\n"
   ],
   "id": "d56e5b1d79899245",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "351069185f461309",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-15T18:41:59.600263Z",
     "start_time": "2025-12-15T18:41:48.013846Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ---- Paths (edit these) ----\n",
    "SFT_JSONL_PATH = Path(\"../data/merged_sft_jokes.jsonl\")          # or your local path\n",
    "DPO_CSV_PATH   = Path(\"../data/generated_data/dpo_final_set.csv\")              # or your local path\n",
    "\n",
    "# Output of this step\n",
    "MERGED_OUT_PATH = Path(\"merged_for_anchor_sampling.parquet\")      # fast intermediate\n",
    "FINAL_OUT_PATH  = Path(\"anchors_dataset.csv\")                     # final requested output\n",
    "\n",
    "\n",
    "def normalize_one_line(s: str) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def load_sft_jsonl(path: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "\n",
    "            # supports either schema:\n",
    "            # - setup/punchline\n",
    "            # - prompt/response\n",
    "            setup = obj.get(\"setup\")\n",
    "            punchline = obj.get(\"punchline\")\n",
    "            if setup is None and punchline is None:\n",
    "                setup = obj.get(\"prompt\", \"\")\n",
    "                punchline = obj.get(\"response\", \"\")\n",
    "\n",
    "            setup = normalize_one_line(setup)\n",
    "            punchline = normalize_one_line(punchline)\n",
    "\n",
    "            if setup and punchline:\n",
    "                rows.append({\"setup\": setup, \"punchline\": punchline, \"source\": \"sft\"})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def load_dpo_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # expected: setup + chosen_punchline\n",
    "    if \"setup\" not in df.columns or \"chosen_punchline\" not in df.columns:\n",
    "        raise ValueError(f\"Expected columns setup + chosen_punchline in: {path}\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"setup\": df[\"setup\"].map(normalize_one_line),\n",
    "        \"punchline\": df[\"chosen_punchline\"].map(normalize_one_line),\n",
    "        \"source\": \"dpo_chosen\",\n",
    "    })\n",
    "\n",
    "    out = out[(out[\"setup\"] != \"\") & (out[\"punchline\"] != \"\")]\n",
    "    return out\n",
    "\n",
    "\n",
    "df_sft = load_sft_jsonl(SFT_JSONL_PATH)\n",
    "df_dpo = load_dpo_csv(DPO_CSV_PATH)\n",
    "\n",
    "df = pd.concat([df_sft, df_dpo], ignore_index=True)\n",
    "df[\"joke\"] = (df[\"setup\"] + \" \" + df[\"punchline\"]).map(normalize_one_line)\n",
    "\n",
    "# basic cleanup\n",
    "df = df[df[\"joke\"] != \"\"].drop_duplicates(subset=[\"setup\", \"punchline\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"SFT rows:\", len(df_sft))\n",
    "print(\"DPO chosen rows:\", len(df_dpo))\n",
    "print(\"Merged unique rows:\", len(df))\n",
    "\n",
    "# optional: save intermediate (fast reload)\n",
    "df.to_parquet(MERGED_OUT_PATH, index=False)\n",
    "print(\"Saved:\", MERGED_OUT_PATH)\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:42:26.119703Z",
     "start_time": "2025-12-15T18:42:24.525666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you already have spaCy and the model, you can skip the installs.\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "except Exception:\n",
    "    raise RuntimeError(\"Install spaCy first: pip install spacy\")\n",
    "\n",
    "try:\n",
    "    _NLP = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    raise RuntimeError(\"Download the model: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "print(\"spaCy loaded:\", _NLP)\n"
   ],
   "id": "cd45f690361d49a1",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T18:42:27.674279Z",
     "start_time": "2025-12-15T18:42:27.667469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "GENERIC_NOUNS = {\n",
    "    \"thing\", \"things\", \"stuff\", \"something\", \"anything\", \"everything\",\n",
    "    \"someone\", \"anyone\", \"everyone\", \"somebody\", \"anybody\", \"everybody\",\n",
    "    \"person\", \"people\", \"man\", \"men\", \"woman\", \"women\", \"guy\", \"guys\", \"girl\", \"girls\", \"kid\", \"kids\",\n",
    "    \"friend\", \"friends\", \"family\",\n",
    "    \"time\", \"day\", \"week\", \"month\", \"year\", \"moment\",\n",
    "    \"place\", \"home\", \"house\", \"room\",\n",
    "    \"job\", \"work\", \"boss\", \"company\",\n",
    "    \"way\", \"lot\", \"kind\", \"sort\", \"part\", \"case\", \"point\", \"problem\", \"idea\", \"fact\", \"question\", \"answer\",\n",
    "    \"joke\", \"story\",\n",
    "}\n",
    "\n",
    "STOPWORDS = {\n",
    "    \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"if\", \"then\", \"else\", \"when\", \"while\",\n",
    "    \"to\", \"of\", \"in\", \"on\", \"for\", \"at\", \"by\", \"with\", \"from\", \"as\",\n",
    "    \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "    \"it\", \"its\", \"this\", \"that\", \"these\", \"those\",\n",
    "    \"i\", \"me\", \"my\", \"mine\", \"you\", \"your\", \"yours\", \"we\", \"our\", \"ours\", \"they\", \"their\", \"theirs\",\n",
    "    \"he\", \"him\", \"his\", \"she\", \"her\", \"hers\",\n",
    "    \"do\", \"does\", \"did\", \"doing\",\n",
    "    \"not\", \"no\", \"yes\",\n",
    "}\n",
    "\n",
    "def base_form_for_similarity(w: str) -> str:\n",
    "    x = w.lower()\n",
    "    if x.endswith(\"ies\") and len(x) > 4:\n",
    "        return x[:-3] + \"y\"\n",
    "    if x.endswith(\"s\") and not x.endswith(\"ss\") and len(x) > 3:\n",
    "        return x[:-1]\n",
    "    return x\n",
    "\n",
    "def noun_candidates(text: str, *, min_len: int = 3, prefer_common_nouns: bool = True) -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Returns list of (surface, weight).\n",
    "    Surface keeps original casing for exact-match constraints.\n",
    "    Weight down-weights proper nouns if prefer_common_nouns is True.\n",
    "    \"\"\"\n",
    "    text = normalize_one_line(text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    doc = _NLP(text)\n",
    "    out = []\n",
    "    seen = set()\n",
    "\n",
    "    for tok in doc:\n",
    "        if tok.pos_ not in {\"NOUN\", \"PROPN\"}:\n",
    "            continue\n",
    "\n",
    "        surface = tok.text.strip()\n",
    "        if len(surface) < min_len:\n",
    "            continue\n",
    "        if not surface.isalpha():\n",
    "            continue\n",
    "\n",
    "        lower = surface.lower()\n",
    "        if len(lower) < min_len:\n",
    "            continue\n",
    "        if lower in STOPWORDS or lower in GENERIC_NOUNS:\n",
    "            continue\n",
    "        if lower in seen:\n",
    "            continue\n",
    "\n",
    "        is_proper = (tok.pos_ == \"PROPN\")\n",
    "        weight = 1.0\n",
    "        if prefer_common_nouns and is_proper:\n",
    "            weight = 0.25\n",
    "\n",
    "        out.append((surface, weight))\n",
    "        seen.add(lower)\n",
    "\n",
    "    return out\n",
    "\n",
    "def choose_two_anchors(setup: str, punchline: str, *, seed: int, prefer_common_nouns: bool = True) -> tuple[str, str] | None:\n",
    "    rnd = random.Random(seed)\n",
    "\n",
    "    setup_c = noun_candidates(setup, prefer_common_nouns=prefer_common_nouns)\n",
    "    punch_c = noun_candidates(punchline, prefer_common_nouns=prefer_common_nouns)\n",
    "\n",
    "    if not setup_c and not punch_c:\n",
    "        return None\n",
    "\n",
    "    def pick_one(cands):\n",
    "        if not cands:\n",
    "            return None\n",
    "        words, weights = zip(*cands)\n",
    "        return rnd.choices(words, weights=weights, k=1)[0]\n",
    "\n",
    "    # mix: 50% one+one, 25% two-setup, 25% two-punchline\n",
    "    r = rnd.random()\n",
    "    if r < 0.50:\n",
    "        a = pick_one(setup_c) or pick_one(punch_c)\n",
    "        b = pick_one(punch_c) or pick_one(setup_c)\n",
    "    elif r < 0.75:\n",
    "        a = pick_one(setup_c) or pick_one(punch_c)\n",
    "        b = pick_one(setup_c) or pick_one(punch_c)\n",
    "    else:\n",
    "        a = pick_one(punch_c) or pick_one(setup_c)\n",
    "        b = pick_one(punch_c) or pick_one(setup_c)\n",
    "\n",
    "    if not a or not b:\n",
    "        return None\n",
    "    if a == b:\n",
    "        return None\n",
    "    if base_form_for_similarity(a) == base_form_for_similarity(b):\n",
    "        return None\n",
    "\n",
    "    return (a, b)\n"
   ],
   "id": "1126a80c2597116e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T22:05:43.597872Z",
     "start_time": "2025-12-15T18:42:29.768743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reload merged table if you want:\n",
    "# df = pd.read_parquet(MERGED_OUT_PATH)\n",
    "\n",
    "BASE_SEED = 1337\n",
    "ROWS_PER_JOKE_TARGET = 2          # try 2; if not possible it will keep 1\n",
    "MAX_TRIES_PER_ROW = 40            # resampling tries\n",
    "\n",
    "out_rows = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    setup = row[\"setup\"]\n",
    "    punchline = row[\"punchline\"]\n",
    "    joke = row[\"joke\"]\n",
    "\n",
    "    used_pairs = set()\n",
    "\n",
    "    produced = 0\n",
    "    for k in range(ROWS_PER_JOKE_TARGET):\n",
    "        pair = None\n",
    "\n",
    "        for t in range(MAX_TRIES_PER_ROW):\n",
    "            seed = BASE_SEED + (i * 10_000) + (k * 100) + t\n",
    "            pair = choose_two_anchors(setup, punchline, seed=seed, prefer_common_nouns=True)\n",
    "            if pair is None:\n",
    "                continue\n",
    "\n",
    "            a1, a2 = pair\n",
    "            # unordered dedup per joke\n",
    "            key = \"||\".join(sorted([a1.lower(), a2.lower()]))\n",
    "            if key in used_pairs:\n",
    "                pair = None\n",
    "                continue\n",
    "\n",
    "            used_pairs.add(key)\n",
    "            break\n",
    "\n",
    "        if pair is None:\n",
    "            continue\n",
    "\n",
    "        a1, a2 = pair\n",
    "        out_rows.append({\n",
    "            \"anchor1\": a1,\n",
    "            \"anchor2\": a2,\n",
    "            \"joke\": joke,\n",
    "        })\n",
    "        produced += 1\n",
    "\n",
    "    # if we could not produce any anchors, skip the joke entirely\n",
    "    # (this happens when no nouns survive filtering)\n",
    "    if produced == 0:\n",
    "        continue\n",
    "\n",
    "df_out = pd.DataFrame(out_rows)\n",
    "\n",
    "print(\"Final rows:\", len(df_out))\n",
    "print(df_out.head(10))\n",
    "\n",
    "df_out.to_csv(FINAL_OUT_PATH, index=False)\n",
    "print(\"Saved:\", FINAL_OUT_PATH)\n"
   ],
   "id": "327f2a82fb701213",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T22:36:47.416167Z",
     "start_time": "2025-12-15T22:36:47.404139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import importlib.util\n",
    "\n",
    "PB_PATH = Path(\"../inference/task_a/two_words/prompt_builder_two_words.py\")  # change if needed\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"prompt_builder_two_words\", str(PB_PATH))\n",
    "pb = importlib.util.module_from_spec(spec)\n",
    "assert spec.loader is not None\n",
    "spec.loader.exec_module(pb)\n",
    "\n",
    "print(\"Loaded:\", PB_PATH)\n"
   ],
   "id": "75a83019afc7fc95",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T22:36:52.933911Z",
     "start_time": "2025-12-15T22:36:52.927994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from functools import lru_cache\n",
    "\n",
    "WIKI_HEADERS = {\n",
    "    \"User-Agent\": \"MWAHAHA/1.0 (contact: dardemtum@gmail.com) humor-generation\"\n",
    "}\n",
    "\n",
    "# Replace the function in-memory (no file editing needed)\n",
    "@lru_cache(maxsize=2048)\n",
    "def get_wikipedia_extract_cached_with_ua(word: str) -> str:\n",
    "    w = pb.safe_word(word)\n",
    "    if not w:\n",
    "        return \"\"\n",
    "\n",
    "    url = pb.WIKI_SUMMARY_URL.format(requests.utils.quote(w))\n",
    "    try:\n",
    "        r = requests.get(url, headers=WIKI_HEADERS, timeout=12)\n",
    "        if r.status_code != 200:\n",
    "            return \"\"\n",
    "        data = r.json()\n",
    "        extract = data.get(\"extract\", \"\") or \"\"\n",
    "        return pb.normalize_one_line(extract)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "pb.get_wikipedia_extract_cached = get_wikipedia_extract_cached_with_ua\n",
    "pb.get_wikipedia_extract_cached.cache_clear()\n",
    "\n",
    "print(\"Patched pb.get_wikipedia_extract_cached with User-Agent.\")\n"
   ],
   "id": "e7fa1581747ff267",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T22:36:56.176941Z",
     "start_time": "2025-12-15T22:36:56.121141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "ANCHORS_PATH = Path(\"anchors_dataset.csv\")  # change to your actual file path\n",
    "\n",
    "df = pd.read_csv(ANCHORS_PATH)\n",
    "assert {\"anchor1\", \"anchor2\"}.issubset(df.columns)\n",
    "\n",
    "unique_words = sorted(set(df[\"anchor1\"].astype(str)) | set(df[\"anchor2\"].astype(str)))\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Unique anchors:\", len(unique_words))\n",
    "print(\"Sample:\", unique_words[:20])\n"
   ],
   "id": "239dd14f78086590",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:28:11.105497Z",
     "start_time": "2025-12-15T22:52:54.700319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "extracts = {}\n",
    "\n",
    "SLEEP_SECONDS = 0.25\n",
    "PRINT_EVERY = 50\n",
    "\n",
    "def normalize_anchor_for_wiki(word: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a list of candidates to try on Wikipedia, in order.\n",
    "    Keep the original first, then try de-pluralized / de-inflected variants.\n",
    "    \"\"\"\n",
    "    w = pb.safe_word(word)\n",
    "    if not w:\n",
    "        return []\n",
    "\n",
    "    # Keep original\n",
    "    candidates = [w]\n",
    "\n",
    "    wl = w.lower()\n",
    "\n",
    "    # If all caps (BRIANS), try title case and lower\n",
    "    if w.isupper() and len(w) >= 4:\n",
    "        candidates.append(w.title())\n",
    "        candidates.append(w.lower())\n",
    "\n",
    "    # Simple English inflection fixes (cheap and surprisingly effective)\n",
    "    # bullies -> bully\n",
    "    if wl.endswith(\"ies\") and len(wl) > 4:\n",
    "        candidates.append(w[:-3] + \"y\")\n",
    "\n",
    "    # boxes / watches / classes -> box / watch / class (approx)\n",
    "    if wl.endswith(\"es\") and len(wl) > 4:\n",
    "        candidates.append(w[:-2])\n",
    "\n",
    "    # cats -> cat, astronauts -> astronaut\n",
    "    if wl.endswith(\"s\") and len(wl) > 3 and not wl.endswith(\"ss\"):\n",
    "        candidates.append(w[:-1])\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for c in candidates:\n",
    "        c2 = c.strip()\n",
    "        if not c2:\n",
    "            continue\n",
    "        key = c2.lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(c2)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_extract_with_fallbacks(word: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Returns (extract, used_term). Empty extract => used_term is the last tried candidate.\n",
    "    Uses pb.get_wikipedia_extract_cached (already patched with User-Agent).\n",
    "    \"\"\"\n",
    "    candidates = normalize_anchor_for_wiki(word)\n",
    "    if not candidates:\n",
    "        return (\"\", \"\")\n",
    "\n",
    "    last = candidates[-1]\n",
    "    for cand in candidates:\n",
    "        ex = pb.get_wikipedia_extract_cached(cand)\n",
    "        if ex:\n",
    "            return (ex, cand)\n",
    "        last = cand\n",
    "    return (\"\", last)\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "for i, w in enumerate(unique_words, start=1):\n",
    "    w_clean = pb.safe_word(w)\n",
    "    if not w_clean:\n",
    "        continue\n",
    "\n",
    "    ex, used = get_extract_with_fallbacks(w_clean)\n",
    "    extracts[w_clean] = ex\n",
    "\n",
    "    if i % PRINT_EVERY == 0:\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"{i}/{len(unique_words)} done (elapsed {elapsed:.1f}s). Example word={w_clean!r}, used={used!r}, has_extract={bool(ex)}\")\n",
    "        if ex:\n",
    "            print(ex[:400])\n",
    "\n",
    "    time.sleep(SLEEP_SECONDS)\n",
    "\n",
    "print(\"Done. Non-empty extracts:\", sum(1 for v in extracts.values() if v))\n",
    "\n",
    "CACHE_PATH = Path(\"wiki_extract_cache.json\")\n",
    "CACHE_PATH.write_text(json.dumps(extracts, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved cache:\", CACHE_PATH)\n"
   ],
   "id": "a4167b3635e17836",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:29:55.740915Z",
     "start_time": "2025-12-16T00:29:55.731544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Freeze pb.get_wikipedia_extract_cached to ONLY read from the local dict\n",
    "def get_wikipedia_extract_from_local_cache(word: str) -> str:\n",
    "    w = pb.safe_word(word)\n",
    "    return extracts.get(w, \"\") or \"\"\n",
    "\n",
    "pb.get_wikipedia_extract_cached = get_wikipedia_extract_from_local_cache\n",
    "\n",
    "print(\"pb.get_wikipedia_extract_cached now reads from local cache only.\")\n"
   ],
   "id": "e6836e81b137af2f",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:30:01.147346Z",
     "start_time": "2025-12-16T00:29:59.319045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def facts_block_row(row) -> str:\n",
    "    return pb.format_facts_block(str(row[\"anchor1\"]), str(row[\"anchor2\"]))\n",
    "\n",
    "df[\"facts_block\"] = df.apply(facts_block_row, axis=1)\n",
    "\n",
    "OUT_PATH = Path(\"anchors_with_facts.parquet\")\n",
    "df.to_parquet(OUT_PATH, index=False)\n",
    "print(\"Saved:\", OUT_PATH)\n",
    "\n",
    "# quick preview\n",
    "df[[\"anchor1\", \"anchor2\", \"facts_block\"]].head(3)\n"
   ],
   "id": "c84cba513801883e",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:30:53.631034Z",
     "start_time": "2025-12-16T00:30:53.490527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Save TSV with anchors + facts + joke\n",
    "TSV_PATH = Path(\"anchors_with_facts.tsv\")\n",
    "\n",
    "needed = [\"anchor1\", \"anchor2\", \"facts_block\", \"joke\"]\n",
    "missing = [c for c in needed if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in df: {missing}\")\n",
    "\n",
    "df[needed].to_csv(TSV_PATH, sep=\"\\t\", index=False)\n",
    "print(\"Saved:\", TSV_PATH.resolve())\n"
   ],
   "id": "f9a973b91fe90d7b",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T00:31:14.889008Z",
     "start_time": "2025-12-16T00:31:14.881497Z"
    }
   },
   "cell_type": "code",
   "source": "df[needed]",
   "id": "ea32d4c3be5d6174",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "973267347ba4616f",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

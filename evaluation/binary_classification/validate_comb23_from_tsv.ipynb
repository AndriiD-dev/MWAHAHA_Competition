{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ae3bf6",
   "metadata": {},
   "source": [
    "# Validate `Humor-Research/humor-detection-comb-23` on a TSV file\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Loads a TSV file.\n",
    "2. Feeds the **`prediction`** column (text) into the model `Humor-Research/humor-detection-comb-23`.\n",
    "3. Adds per-row probabilities and predicted labels.\n",
    "4. Summarizes:\n",
    "   - **average certainty for predicted non-jokes (label 0)**\n",
    "   - **average certainty for predicted jokes (label 1)**\n",
    "   - **count of predicted non-jokes and jokes**\n",
    "\n",
    "> Note: The Humor-Research model repositories typically do not include tokenizer files, so we load the tokenizer from `roberta-base` (this is also the usage shown by the authors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba2b61",
   "metadata": {},
   "source": [
    "# If needed, uncomment and run once:\n",
    "# %pip install -q torch transformers pandas tqdm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91a6797",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, RobertaTokenizerFast\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "import transformers\n",
    "print(\"transformers:\", transformers.__version__)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d00344d",
   "metadata": {},
   "source": [
    "# === Configuration ===\n",
    "\n",
    "TSV_FILE = \"task-a-title_predictions_base 6 (all jokes generated)\"\n",
    "TSV_PATH = f\"../../experiment_results_good/{TSV_FILE}.tsv\"   # <-- set your TSV path here\n",
    "TEXT_COLUMN = \"prediction\"             # <-- the column to score with the model\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 256  # 512 is fine too; 256 is usually enough for short jokes\n",
    "\n",
    "OUT_DIR = \"results_exp_good\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "MODEL_ID = \"Humor-Research/humor-detection-comb-23\"\n",
    "TOKENIZER_ID = \"roberta-base\"  # tokenizer comes from roberta-base"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8b756a52",
   "metadata": {},
   "source": [
    "## Load TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf8009a3",
   "metadata": {},
   "source": [
    "if not os.path.exists(TSV_PATH):\n",
    "    raise FileNotFoundError(f\"TSV file not found: {TSV_PATH}\")\n",
    "\n",
    "df = pd.read_csv(TSV_PATH, sep=\"\\t\")\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "if TEXT_COLUMN not in df.columns:\n",
    "    raise KeyError(f\"Column '{TEXT_COLUMN}' not found. Available columns: {list(df.columns)}\")\n",
    "\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e719a7c9",
   "metadata": {},
   "source": [
    "## Load model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6ddfc34",
   "metadata": {},
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(TOKENIZER_ID)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded:\", MODEL_ID)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "42eb81b4",
   "metadata": {},
   "source": [
    "## Score the TSV (probabilities and predictions)\n",
    "\n",
    "We compute:\n",
    "\n",
    "- `p_not_joke` = probability of label 0  \n",
    "- `p_joke` = probability of label 1  \n",
    "- `model_pred` = argmax label (0 or 1)  \n",
    "- `certainty` = probability of the predicted label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7afc369",
   "metadata": {},
   "source": [
    "@torch.inference_mode()\n",
    "def score_texts(texts):\n",
    "    all_p0 = []\n",
    "    all_p1 = []\n",
    "    for start in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Scoring\"):\n",
    "        batch_texts = texts[start:start+BATCH_SIZE]\n",
    "        enc = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        logits = model(**enc).logits  # [batch, 2]\n",
    "        probs = torch.softmax(logits, dim=-1).detach().cpu()  # [batch, 2]\n",
    "        all_p0.extend(probs[:, 0].tolist())\n",
    "        all_p1.extend(probs[:, 1].tolist())\n",
    "    return all_p0, all_p1\n",
    "\n",
    "texts = df[TEXT_COLUMN].astype(str).tolist()\n",
    "p0, p1 = score_texts(texts)\n",
    "\n",
    "df[\"p_not_joke\"] = p0\n",
    "df[\"p_joke\"] = p1\n",
    "df[\"model_pred\"] = (df[\"p_joke\"] > df[\"p_not_joke\"]).astype(int)\n",
    "df[\"certainty\"] = df.apply(lambda r: r[\"p_joke\"] if r[\"model_pred\"] == 1 else r[\"p_not_joke\"], axis=1)\n",
    "\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0d9e33a7",
   "metadata": {},
   "source": [
    "## Summary: average certainties and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d9ec51f",
   "metadata": {},
   "source": [
    "# Counts\n",
    "count_not_joke = int((df[\"model_pred\"] == 0).sum())\n",
    "count_joke = int((df[\"model_pred\"] == 1).sum())\n",
    "\n",
    "# Average certainty for each predicted class:\n",
    "avg_not_joke_certainty = float(df[\"p_not_joke\"].mean()) if count_not_joke else float(\"nan\")\n",
    "avg_joke_certainty = float(df[\"p_joke\"].mean()) if count_joke else float(\"nan\")\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\"predicted_label\": 0, \"label_name\": \"not_joke\", \"count\": count_not_joke, \"avg_certainty\": avg_not_joke_certainty},\n",
    "    {\"predicted_label\": 1, \"label_name\": \"joke\", \"count\": count_joke, \"avg_certainty\": avg_joke_certainty},\n",
    "])\n",
    "\n",
    "summary"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "075e427b",
   "metadata": {},
   "source": [
    "print(\"Predicted not_joke count:\", count_not_joke, \"| average certainty:\", avg_not_joke_certainty)\n",
    "print(\"Predicted joke count    :\", count_joke,     \"| average certainty:\", avg_joke_certainty)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "77f29f74",
   "metadata": {},
   "source": [
    "## Save outputs\n",
    "\n",
    "- `output/comb23_scored_with_summary.tsv` â€“ your original TSV plus probabilities and predictions + summary table (counts and average certainties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c60c57f",
   "metadata": {},
   "source": [
    "# Save outputs (single file)\n",
    "\n",
    "# We will write everything to ONE TSV file:\n",
    "# 1) the scored rows (original TSV + probabilities + predictions)\n",
    "# 2) a blank line\n",
    "# 3) a small summary block with counts + average certainties\n",
    "\n",
    "single_path = os.path.join(OUT_DIR, f\"{TSV_FILE}_scored_with_summary.tsv\")\n",
    "\n",
    "# 1) write scored rows\n",
    "df.to_csv(single_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# 2) append summary block\n",
    "with open(single_path, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"# summary\\n\")\n",
    "    f.write(f\"# predicted_not_joke_count\\t{count_not_joke}\\n\")\n",
    "    f.write(f\"# predicted_joke_count\\t{count_joke}\\n\")\n",
    "    f.write(f\"# avg_not_joke_certainty\\t{avg_not_joke_certainty}\\n\")\n",
    "    f.write(f\"# avg_joke_certainty\\t{avg_joke_certainty}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    # also append the summary table as TSV\n",
    "    summary.to_csv(f, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Saved single TSV (scored + summary):\", single_path)\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
